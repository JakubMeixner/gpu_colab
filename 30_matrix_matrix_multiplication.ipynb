{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c549ff7f",
      "metadata": {
        "id": "c549ff7f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggruszczynski/gpu_colab/blob/main/30_matrix_matrix_multiplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dbce82a-6a52-4c23-82af-4b4b8d3252bf",
      "metadata": {
        "id": "6dbce82a-6a52-4c23-82af-4b4b8d3252bf"
      },
      "source": [
        "# Matrix x Matrix multiplication\n",
        "As a step by step instruction has been presented in tutorial 2, here is a time for a stand-alone practice.\n",
        "\n",
        "Accelerate the code - finish the matrix multiplication cuda kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "94474f2e-6bd9-494a-8847-e9d0f0593248",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94474f2e-6bd9-494a-8847-e9d0f0593248",
        "outputId": "42bf8873-66d8-46e8-c2c0-96ca57bf2f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_add.cu\n"
          ]
        }
      ],
      "source": [
        "%%file matrix_add.cu\n",
        "\n",
        "// This program computes a simple version of matrix multiplication\n",
        "// By: Nick from CoffeeBeforeArch\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cassert>\n",
        "#include <cstdlib>\n",
        "#include <functional>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "using std::cout;\n",
        "using std::generate;\n",
        "using std::vector;\n",
        "\n",
        "__global__ void matrixMul(const int *a, const int *b, int *c, int N) {\n",
        "  // Compute each thread's global row and column index\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  // Iterate over row, and down column\n",
        "  c[row * N + col] = 0;\n",
        "  for (int k = 0; k < N; k++) {\n",
        "    // Accumulate results for a single element\n",
        "    // TODO: write your code here\n",
        "  }\n",
        "}\n",
        "\n",
        "// Check result on the CPU\n",
        "void verify_result(vector<int> &a, vector<int> &b, vector<int> &c, int N) {\n",
        "  for (int row = 0; row < N; row++) {\n",
        "    for (int col = 0; col < N; col++) {\n",
        "      int tmp = 0; // For every element in the row-column pair\n",
        "      for (int k = 0; k < N; k++) {\n",
        "        // Accumulate the partial results\n",
        "        tmp += a[row * N + k] * b[k * N + col];\n",
        "      }\n",
        "      // Check against the CPU result\n",
        "      assert(tmp == c[row * N + col]);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 1 << 10;  // Matrix size of 1024 x 1024;\n",
        "\n",
        "  // Size (in bytes) of matrix\n",
        "  size_t bytes = N * N * sizeof(int);\n",
        "\n",
        "  // Host vectors\n",
        "  vector<int> h_a(N * N);\n",
        "  vector<int> h_b(N * N);\n",
        "  vector<int> h_c(N * N);\n",
        "\n",
        "  // Initialize matrices\n",
        "  generate(h_a.begin(), h_a.end(), []() { return rand() % 100; });\n",
        "  generate(h_b.begin(), h_b.end(), []() { return rand() % 100; });\n",
        "\n",
        "  // Allocate device memory\n",
        "  int *d_a, *d_b, *d_c;\n",
        "  cudaMalloc(&d_a, bytes);\n",
        "  cudaMalloc(&d_b, bytes);\n",
        "  cudaMalloc(&d_c, bytes);\n",
        "\n",
        "  // Copy data to the device\n",
        "  cudaMemcpy(d_a, h_a.data(), bytes, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, h_b.data(), bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Threads per CTA dimension\n",
        "  int THREADS = 32;\n",
        "\n",
        "  // Blocks per grid dimension (assumes THREADS divides N evenly)\n",
        "  int BLOCKS = N / THREADS;\n",
        "\n",
        "  // Use dim3 structs for block  and grid dimensions\n",
        "  dim3 threads(THREADS, THREADS);\n",
        "  dim3 blocks(BLOCKS, BLOCKS);\n",
        "\n",
        "  // Launch kernel\n",
        "  matrixMul<<<blocks, threads>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "  // Copy back to the host\n",
        "  cudaMemcpy(h_c.data(), d_c, bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Check result\n",
        "  verify_result(h_a, h_b, h_c, N);\n",
        "\n",
        "  cout << \"COMPLETED SUCCESSFULLY\\n\";\n",
        "\n",
        "  // Free memory on device\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "930c74a3-f35a-4350-8e8f-2ba0df9828d2",
      "metadata": {
        "tags": [],
        "id": "930c74a3-f35a-4350-8e8f-2ba0df9828d2",
        "outputId": "6ede04fc-6ad7-461d-c2ad-25d6239f63f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check your GPU version\n",
            "Sat Oct 28 11:58:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!echo \"Check your GPU version\"\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "491b45f5-e42a-473c-9448-5a1ecd58bd84",
      "metadata": {
        "tags": [],
        "id": "491b45f5-e42a-473c-9448-5a1ecd58bd84",
        "outputId": "a69dd5a7-ebe0-4b00-c264-c77ce3e9883b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPLETED SUCCESSFULLY\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "CUDA_SUFF=70 # or CUDA_SUFF=35 for older GPUs\n",
        "nvcc -gencode arch=compute_${CUDA_SUFF},code=sm_${CUDA_SUFF} ./matrix_add.cu -o matrix_add\n",
        "./matrix_add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2a2e721a-4930-489a-8e03-f4027eabc5ae",
      "metadata": {
        "tags": [],
        "id": "2a2e721a-4930-489a-8e03-f4027eabc5ae",
        "outputId": "6910a9b1-b5b2-464d-ee8f-75e80cac7fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPLETED SUCCESSFULLY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==1365== NVPROF is profiling process 1365, command: ./matrix_add\n",
            "==1365== Profiling application: ./matrix_add\n",
            "==1365== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   86.82%  14.491ms         1  14.491ms  14.491ms  14.491ms  matrixMul(int const *, int const *, int*, int)\n",
            "                    9.39%  1.5670ms         2  783.48us  743.80us  823.16us  [CUDA memcpy HtoD]\n",
            "                    3.79%  633.34us         1  633.34us  633.34us  633.34us  [CUDA memcpy DtoH]\n",
            "      API calls:   93.85%  293.91ms         3  97.972ms  68.515us  293.77ms  cudaMalloc\n",
            "                    5.60%  17.526ms         3  5.8420ms  978.63us  15.553ms  cudaMemcpy\n",
            "                    0.28%  861.92us         1  861.92us  861.92us  861.92us  cuDeviceGetPCIBusId\n",
            "                    0.20%  640.75us         3  213.58us  205.89us  226.45us  cudaFree\n",
            "                    0.05%  164.87us       101  1.6320us     190ns  66.125us  cuDeviceGetAttribute\n",
            "                    0.01%  31.055us         1  31.055us  31.055us  31.055us  cudaLaunchKernel\n",
            "                    0.01%  26.863us         1  26.863us  26.863us  26.863us  cuDeviceGetName\n",
            "                    0.00%  2.0720us         3     690ns     331ns  1.2800us  cuDeviceGetCount\n",
            "                    0.00%  1.3010us         2     650ns     298ns  1.0030us  cuDeviceGet\n",
            "                    0.00%     720ns         1     720ns     720ns     720ns  cuDeviceTotalMem\n",
            "                    0.00%     534ns         1     534ns     534ns     534ns  cuModuleGetLoadingMode\n",
            "                    0.00%     382ns         1     382ns     382ns     382ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# ls\n",
        "\n",
        "nvprof  ./matrix_add"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09eb0262-fb91-4a40-a0ea-26eeb6a18b70",
      "metadata": {
        "id": "09eb0262-fb91-4a40-a0ea-26eeb6a18b70"
      },
      "source": [
        "### What is the difference between ‘GPU activities’ and ‘API calls’ in the results of ‘nvprof’?\n",
        "\n",
        "Answer from <https://forums.developer.nvidia.com/t/what-is-the-difference-between-gpu-activities-and-api-calls-in-the-results-of-nvprof/71338/1>\n",
        "\n",
        "Section ‘GPU activities’ list activities which execute on the GPU like CUDA kernel, CUDA memcpy, CUDA memset. And timing information here represents the execution time on the GPU.\n",
        "\n",
        "Section ‘API Calls’ list CUDA Runtime/Driver API calls. And timing information here represents the execution time on the host.\n",
        "\n",
        "For example, CUDA kernel launches are asynchronous from the point of view of the CPU.\n",
        "It returns immediately, before the kernel has completed, and perhaps before the kernel has even started.\n",
        "This time is captured for the Launch API like cuLaunchKernel in the ‘API Calls’ section.\n",
        "Eventually kernel starts execution on the GPU and runs to the completion.\n",
        "This time is captured for kernel in the ‘GPU activities’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "187ee37b-8283-45be-acc7-68866ec03372",
      "metadata": {
        "tags": [],
        "id": "187ee37b-8283-45be-acc7-68866ec03372",
        "outputId": "b5076c63-015e-450a-de2b-998b525ad3e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPLETED SUCCESSFULLY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==1459== NVPROF is profiling process 1459, command: ./matrix_add --benchmark\n",
            "==1459== Profiling application: ./matrix_add --benchmark\n",
            "==1459== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "364.99ms  819.58us                    -               -         -         -         -  4.0000MB  4.7662GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "366.05ms  753.69us                    -               -         -         -         -  4.0000MB  5.1828GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "366.81ms  14.434ms            (32 32 1)       (32 32 1)        28        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  matrixMul(int const *, int const *, int*, int) [117]\n",
            "381.26ms  653.47us                    -               -         -         -         -  4.0000MB  5.9777GB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "nvprof --print-gpu-trace ./matrix_add --benchmark"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}