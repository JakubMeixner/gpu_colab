{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c549ff7f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ggruszczynski/gpu_colab/blob/main/30_matrix_matrix_multiplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbce82a-6a52-4c23-82af-4b4b8d3252bf",
   "metadata": {},
   "source": [
    "# Matrix x Matrix multiplication\n",
    "As a step by step instruction has been presented in tutorial 2, here is a time for a stand-alone practice.\n",
    "\n",
    "Accelerate the serial, element-wise square matrix addition code using cuda kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94474f2e-6bd9-494a-8847-e9d0f0593248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matrix_add.cu\n"
     ]
    }
   ],
   "source": [
    "%%file matrix_add.cu\n",
    "\n",
    "// This program computes a simple version of matrix multiplication\n",
    "// By: Nick from CoffeeBeforeArch\n",
    "\n",
    "#include <algorithm>\n",
    "#include <cassert>\n",
    "#include <cstdlib>\n",
    "#include <functional>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "using std::cout;\n",
    "using std::generate;\n",
    "using std::vector;\n",
    "\n",
    "__global__ void matrixMul(const int *a, const int *b, int *c, int N) {\n",
    "  // Compute each thread's global row and column index\n",
    "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "  // Iterate over row, and down column\n",
    "  c[row * N + col] = 0;\n",
    "  for (int k = 0; k < N; k++) {\n",
    "    // Accumulate results for a single element\n",
    "    c[row * N + col] += a[row * N + k] * b[k * N + col];\n",
    "  }\n",
    "}\n",
    "\n",
    "// Check result on the CPU\n",
    "void verify_result(vector<int> &a, vector<int> &b, vector<int> &c, int N) {\n",
    "  for (int row = 0; row < N; row++) {\n",
    "    for (int col = 0; col < N; col++) {\n",
    "      int tmp = 0; // For every element in the row-column pair\n",
    "      for (int k = 0; k < N; k++) {\n",
    "        // Accumulate the partial results\n",
    "        tmp += a[row * N + k] * b[k * N + col];\n",
    "      }\n",
    "      // Check against the CPU result\n",
    "      assert(tmp == c[row * N + col]);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  int N = 1 << 10;  // Matrix size of 1024 x 1024;\n",
    "\n",
    "  // Size (in bytes) of matrix\n",
    "  size_t bytes = N * N * sizeof(int);\n",
    "\n",
    "  // Host vectors\n",
    "  vector<int> h_a(N * N);\n",
    "  vector<int> h_b(N * N);\n",
    "  vector<int> h_c(N * N);\n",
    "\n",
    "  // Initialize matrices\n",
    "  generate(h_a.begin(), h_a.end(), []() { return rand() % 100; });\n",
    "  generate(h_b.begin(), h_b.end(), []() { return rand() % 100; });\n",
    "\n",
    "  // Allocate device memory\n",
    "  int *d_a, *d_b, *d_c;\n",
    "  cudaMalloc(&d_a, bytes);\n",
    "  cudaMalloc(&d_b, bytes);\n",
    "  cudaMalloc(&d_c, bytes);\n",
    "\n",
    "  // Copy data to the device\n",
    "  cudaMemcpy(d_a, h_a.data(), bytes, cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy(d_b, h_b.data(), bytes, cudaMemcpyHostToDevice);\n",
    "\n",
    "  // Threads per CTA dimension\n",
    "  int THREADS = 32;\n",
    "\n",
    "  // Blocks per grid dimension (assumes THREADS divides N evenly)\n",
    "  int BLOCKS = N / THREADS;\n",
    "\n",
    "  // Use dim3 structs for block  and grid dimensions\n",
    "  dim3 threads(THREADS, THREADS);\n",
    "  dim3 blocks(BLOCKS, BLOCKS);\n",
    "\n",
    "  // Launch kernel\n",
    "  matrixMul<<<blocks, threads>>>(d_a, d_b, d_c, N);\n",
    "\n",
    "  // Copy back to the host\n",
    "  cudaMemcpy(h_c.data(), d_c, bytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  // Check result\n",
    "  verify_result(h_a, h_b, h_c, N);\n",
    "\n",
    "  cout << \"COMPLETED SUCCESSFULLY\\n\";\n",
    "\n",
    "  // Free memory on device\n",
    "  cudaFree(d_a);\n",
    "  cudaFree(d_b);\n",
    "  cudaFree(d_c);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930c74a3-f35a-4350-8e8f-2ba0df9828d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check your GPU version\n",
      "Sun Apr 16 17:09:24 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   56C    P5    29W / 250W |   1465MiB /  8192MiB |     28%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1703      G   /usr/lib/xorg/Xorg                 96MiB |\n",
      "|    0   N/A  N/A      2624      G   /usr/lib/xorg/Xorg                704MiB |\n",
      "|    0   N/A  N/A      2819      G   /usr/bin/gnome-shell               91MiB |\n",
      "|    0   N/A  N/A      3712      G   ...RendererForSitePerProcess        3MiB |\n",
      "|    0   N/A  N/A      8175      G   ...features=BackForwardCache       10MiB |\n",
      "|    0   N/A  N/A      8543      G   ...957248867340528764,131072      156MiB |\n",
      "|    0   N/A  N/A     13016      G   ...AAAAAAAAA= --shared-files       35MiB |\n",
      "|    0   N/A  N/A     14339      G   ...b/thunderbird/thunderbird      137MiB |\n",
      "|    0   N/A  N/A     15643      G   ...RendererForSitePerProcess      181MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!echo \"Check your GPU version\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491b45f5-e42a-473c-9448-5a1ecd58bd84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "CUDA_SUFF=70 # or CUDA_SUFF=35\n",
    "nvcc -gencode arch=compute_${CUDA_SUFF},code=sm_${CUDA_SUFF} ./matrix_add.cu -o matrix_add\n",
    "./matrix_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2e721a-4930-489a-8e03-f4027eabc5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==28468== NVPROF is profiling process 28468, command: ./matrix_add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED SUCCESSFULLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==28468== Profiling application: ./matrix_add\n",
      "==28468== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   79.46%  5.1380ms         1  5.1380ms  5.1380ms  5.1380ms  matrixMul(int const *, int const *, int*, int)\n",
      "                   13.46%  870.09us         2  435.04us  429.16us  440.93us  [CUDA memcpy HtoD]\n",
      "                    7.08%  457.92us         1  457.92us  457.92us  457.92us  [CUDA memcpy DtoH]\n",
      "      API calls:   92.83%  93.226ms         3  31.075ms  45.003us  93.136ms  cudaMalloc\n",
      "                    6.73%  6.7563ms         3  2.2521ms  470.65us  5.7748ms  cudaMemcpy\n",
      "                    0.32%  323.92us         3  107.97us  99.012us  113.98us  cudaFree\n",
      "                    0.07%  67.404us       101     667ns      84ns  28.975us  cuDeviceGetAttribute\n",
      "                    0.02%  21.152us         1  21.152us  21.152us  21.152us  cudaLaunchKernel\n",
      "                    0.02%  18.594us         1  18.594us  18.594us  18.594us  cuDeviceGetName\n",
      "                    0.00%  4.6960us         1  4.6960us  4.6960us  4.6960us  cuDeviceGetPCIBusId\n",
      "                    0.00%  1.3480us         3     449ns     125ns  1.0780us  cuDeviceGetCount\n",
      "                    0.00%  1.1780us         2     589ns      88ns  1.0900us  cuDeviceGet\n",
      "                    0.00%     330ns         1     330ns     330ns     330ns  cuDeviceTotalMem\n",
      "                    0.00%     257ns         1     257ns     257ns     257ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# ls\n",
    "# nvprof  ./matrix_add\n",
    "nvprof  ./matrix_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb0262-fb91-4a40-a0ea-26eeb6a18b70",
   "metadata": {},
   "source": [
    "### What is the difference between ‘GPU activities’ and ‘API calls’ in the results of ‘nvprof’?\n",
    "\n",
    "Answer from <https://forums.developer.nvidia.com/t/what-is-the-difference-between-gpu-activities-and-api-calls-in-the-results-of-nvprof/71338/1>\n",
    "\n",
    "Section ‘GPU activities’ list activities which execute on the GPU like CUDA kernel, CUDA memcpy, CUDA memset. And timing information here represents the execution time on the GPU.\n",
    "\n",
    "Section ‘API Calls’ list CUDA Runtime/Driver API calls. And timing information here represents the execution time on the host.\n",
    "\n",
    "For example, CUDA kernel launches are asynchronous from the point of view of the CPU. \n",
    "It returns immediately, before the kernel has completed, and perhaps before the kernel has even started. \n",
    "This time is captured for the Launch API like cuLaunchKernel in the ‘API Calls’ section. \n",
    "Eventually kernel starts execution on the GPU and runs to the completion. \n",
    "This time is captured for kernel in the ‘GPU activities’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187ee37b-8283-45be-acc7-68866ec03372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==28485== NVPROF is profiling process 28485, command: ./matrix_add --benchmark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED SUCCESSFULLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==28485== Profiling application: ./matrix_add --benchmark\n",
      "==28485== Profiling result:\n",
      "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
      "276.36ms  441.83us                    -               -         -         -         -  4.0000MB  8.8411GB/s    Pageable      Device  NVIDIA GeForce          1         7  [CUDA memcpy HtoD]\n",
      "276.88ms  428.29us                    -               -         -         -         -  4.0000MB  9.1205GB/s    Pageable      Device  NVIDIA GeForce          1         7  [CUDA memcpy HtoD]\n",
      "277.32ms  5.2855ms            (32 32 1)       (32 32 1)        28        0B        0B         -           -           -           -  NVIDIA GeForce          1         7  matrixMul(int const *, int const *, int*, int) [116]\n",
      "282.60ms  444.36us                    -               -         -         -         -  4.0000MB  8.7908GB/s      Device    Pageable  NVIDIA GeForce          1         7  [CUDA memcpy DtoH]\n",
      "\n",
      "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
      "SSMem: Static shared memory allocated per CUDA block.\n",
      "DSMem: Dynamic shared memory allocated per CUDA block.\n",
      "SrcMemType: The type of source memory accessed by memory operation/copy\n",
      "DstMemType: The type of destination memory accessed by memory operation/copy\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --print-gpu-trace ./matrix_add --benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62ad63-61cc-4493-83b0-e32d1955892d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
